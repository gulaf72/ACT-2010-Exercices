\documentclass{article} 
\usepackage{graphicx}
\usepackage[francais]{babel} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage{amsmath} 
\usepackage{amsfonts}
\usepackage{verbatim} 
\usepackage{float} 
\usepackage{hyperref}
\usepackage{scrtime}
\usepackage[scale=2]{ccicons}
\usepackage{tabularx}
\usepackage{Sweave}
\begin{document}
\input{serie2-solutions-concordance}
\input{pagetitre}
\addtocounter{section}{1}
\section{Modèles classiques pour séries chronologiques}

\subsection{Zone de stationnarité pour AR(2) (Théorique)}

On obtient les racines de l'équation caractéristique en utilisant la formule quadratique habituelle

\begin{align*}
\frac{\phi_1\pm\sqrt{\phi_1^2+4\phi_2}}{-2\phi_2}
\end{align*}

On considère les inverses des deux racines, $A_1$ et $A_2$.

\begin{align*}
A_1 &= \frac{2\phi_2}{-\phi_1-\sqrt{\phi_1^2+4\phi_2}} \\
&= \frac{2\phi_2}{-\phi_1-\sqrt{\phi_1^2+4\phi_2}} \left[\frac{-\phi_1+\sqrt{\phi_1^2+4\phi_2}}{-\phi_1+\sqrt{\phi_1^2+4\phi_2}} \right] \\
&= \frac{2\phi_2(-\phi_1+\sqrt{\phi_1^2+4\phi_2})}{\phi_1^2-(\phi_1^2+4\phi_2)}\\
&= \frac{\phi_1-\sqrt{\phi_1^2+4\phi_2}}{2}\\
A_2 &= \frac{\phi_1+\sqrt{\phi_1^2+4\phi_2}}{2}
\end{align*}

Il y a 2 situations possibles: soit les racines sont réelles ($\phi_1^2+4\phi_2>0$) ou elles sont complexes ($\phi_1^2+4\phi_2<0$).
\begin{itemize}
\item \textbf{Racines réelles:}
Comme les racines doivent être plus grandes que 1, alors nécessairement leurs inverses $|A_1|<1$ et $|A_2|<1$. Nous avons donc:
\begin{align*}
-1 &< \frac{\phi_1-\sqrt{\phi_1^2+4\phi_2}}{2} <  \frac{\phi_1+\sqrt{\phi_1^2+4\phi_2}}{2} < 1 \\
\Leftrightarrow -2 &< \phi_1-\sqrt{\phi_1^2+4\phi_2} < \phi_1+\sqrt{\phi_1^2+4\phi_2} < 2
\end{align*}
En observant la première inégalité, on a:
\begin{align*}
-2 < \phi_1-\sqrt{\phi_1^2+4\phi_2}
&\Leftrightarrow \sqrt{\phi_1^2+4\phi_2}<\phi_1+2 \\
&\Leftrightarrow \phi_1^2+4\phi_2 < \phi_1^2+4\phi_1+4 \\
&\Leftrightarrow \phi_2 < \phi_1 + 1 \\
&\Leftrightarrow \phi_2 - \phi_1 < 1
\end{align*}
Ce qui correspond à la seconde condition. En considérant la seconde inégalité, on obtient, de la même façon, la première inégalité:
\begin{align*}
\phi_1+\sqrt{\phi_1^2+4\phi_2} &< 2 \\
&\Leftrightarrow \phi_1 + \phi_2 < 1
\end{align*}
Ces deux conditions réunies avec un discriminant positif forment la région de stationnarité pour des racines réelles.

\item \textbf{Racines complexes:}  
On considère la situation où $\phi_1^2+4\phi_2<0$. Ici, on aura des conjugués complexes et $|A_1| = |A_2| <1$ seulement si $|A_1|^2<1$.
\begin{align*}
|A_1|^2 &=\frac{\phi_1^2+(-\phi_1^2-4\phi_2}{4}=-\phi^2 \\
&\Leftrightarrow \phi_2>-1 \\
&\Leftrightarrow |\phi_2|<1
\end{align*}
Ce résultat réuni avec un discriminant négatif forment la région de stationnarité pour des racines complexes.
\end{itemize}

\clearpage
\subsection{Ordre d'intégration (Théorique)}

\begin{enumerate}

\item On obtient ce résultat par récurrence. Par exemple, pour $k=2$, on a :

\begin{align*}
(1-B)^2 (a_0+a_1t+a_2t^2) &= (1-B) ((a_0+a_1t+a_2t^2)\\ &\quad- (a_0+a_1(t-1)+a_2(t-1)^2)) \\
&= (1-B) (a_1+a_2(2t+1)) \\
&= (a_1+a_2(2t+1)) - (a_1+a_2(2(t-1)+1)) \\
&= 2a_2
\end{align*}

En général, on obtient:

\begin{align*}
(1-B)^k (a_0+a_1t+a_2t^2+\ldots+a_kt^k) &= (1-B)^{k-1} ((a_0+a_1t+a_2t^2+\ldots+a_kt^k)\\ &\quad- (a_0+a_1(t-1)+a_2(t-1)^2+\ldots+a_k(t-1)^k)) \\
&= (1-B)^{k-1} (a_1 + 2a_2t+\ldots+a_k(t^k-(t-1)^k))
\end{align*}

On remarque qu'à chaque itération, le premier terme de la série disparait. Ainsi, après $k$ itérations, il ne restera que le terme en $a_k$ avec son coefficient, qui correspont à $k!$. On obtient ainsi la solution générale.

\item Une série est dite stationnaire lorsque chaque terme est un terme d'erreur dont la distribution est constante au fil du temps. Ainsi, la distribution de la différence de deux termes consécutifs de la série sera aussi constante au fil du temps. Par exemple:
\begin{align*}
\epsilon_t \sim N(0,\sigma^2)  \\
\epsilon_t - \epsilon_{t-1} \sim N(0,2\sigma^2) \\
\end{align*}

\end{enumerate}



\clearpage
\subsection{Inversion de processus d'ordre 1 (Théorique)}

\begin{enumerate}
\item Un processus AR(1) est défini par $y_t = \gamma_1y_{t-1} + \epsilon_t$. En développant le terme $y_{t-1}$, on obtient $y_t = \gamma_1^2y_{t-2}+\gamma_1\epsilon_{t-1}+\epsilon_t$. De manière récursive, on obtient $y_t = \gamma_1^{t}\epsilon_0 + \gamma_1^{t-1}\epsilon_1 + \ldots + \gamma_1\epsilon_{t-1} + \epsilon_t$. ainsi, en faisant tendre $t\to\infty$, on obtient une représentation MA($\infty$).

\item Un processus MA(1) est défini par $y_t = \epsilon_t - \theta_1\epsilon_{t-1}$. On cherche à substituer le terme $\epsilon_{t-1}$. On développe le terme précédent de la série: $y_{t-1} = \epsilon_{t-1} - \theta_1\epsilon_{t-2}$ et on substitue dans la première expression pour obtenir $y_t = \epsilon_t - \theta_1y_{t-1} - \theta_1^2\epsilon_{t-2}$. De manière récursive, on obtient $y_t = -\theta_1^ty_0-\theta_1^{t-1}y_1-\ldots-\theta_1y_{t-1}+\epsilon_{t}$. Lorsque $t\to\infty$, on obtient une représentation AR($\infty$).
\end{enumerate}


\clearpage
\subsection{Construction d'une série autorégressive (Calculatrice)}

On utilise la formule $y_t = \gamma_1y_{t-1} + \epsilon_t$.

\begin{center}
\begin{tabular}{|r|r|r|}
\hline
\multicolumn{1}{|l|}{} & \multicolumn{ 2}{c|}{$\gamma$} \\ \hline
\multicolumn{1}{|l|}{$N(0,1)$} & -0,5 & 0,5 \\ \hline
-1,21 & -1,2100 & -1,2100 \\ \hline
0,28 & 0,8850 & -0,3250 \\ \hline
1,08 & 0,6375 & 0,9175 \\ \hline
-2,35 & -2,6688 & -1,8913 \\ \hline
0,43 & 1,7644 & -0,5156 \\ \hline
0,51 & -0,3722 & 0,2522 \\ \hline
-0,57 & -0,3839 & -0,4439 \\ \hline
-0,55 & -0,3580 & -0,7720 \\ \hline
-0,56 & -0,3810 & -0,9460 \\ \hline
-0,89 & -0,6995 & -1,3630 \\ \hline
\end{tabular}
\end{center}

Les séries avec une corrélation négative ont tendance à aller dans la direction contraire des termes précédents alors que celles avec une corrélation positive ont tendance à aller dans la même direction que les termes précédents.

Le tableur \url{constructionserieAR.ods} contient les calculs effectués.

\subsection{Deux processus MA(2) (Théorique)}

Il suffit de calculer la fonction d'autorégression pour chaque processus MA(2).
Ensuite, on peut évaluer la fonction d'autocorrélation et comparer le résultat obtenu.

\begin{enumerate}
\item Premier processus avec:
\begin{align*}
\theta_1 = \theta_2 = \frac{1}{4}
\end{align*}
Fonction d'autocovariance
\begin{align*}
\gamma_0^{(1)} &= V[Y_t] \\
&= V[e_t]+\frac{1}{16}V[e_{t-1}]+\frac{1}{16}V[e_{t-2}] \\
&= (1+\frac{1}{8})\theta^2_e \\
&= \frac{9}{8} \sigma^2_e \\
\end{align*}
\begin{align*}
\gamma_1^{(1)} &= Cov(Y_t,Y_{t-1}) \\
&= Cov(e_t - \frac{1}{4}e_{t-1} - \frac{1}{4}e_{t-2}, e_{t-1} - \frac{1}{4}e_{t-2} - \frac{1}{4}e_{t-3} )\\
&= Cov(-\frac{1}{4}e_{t-1},e_{t-1}) + Cov(-\frac{1}{4}e_{t-2},-\frac{1}{4}e_{t-2}) \\
&= (-\frac{1}{4}+(-\frac{1}{4})(-\frac{1}{4})) \sigma^2_e \\
&= -\frac{3}{16} \sigma^2_e \\
\end{align*}
\begin{align*}
\gamma_2^{(1)} &= Cov(Y_t,Y_{t-2}) \\
&= Cov(e_t - \frac{1}{4}e_{t-1} - \frac{1}{4}e_{t-2}, e_{t-2} - \frac{1}{4}e_{t-3} - \frac{1}{4}e_{t-4} )\\
&= Cov(-\frac{1}{4}e_{t-2},e_{t-2}) \\
&= -\frac{1}{4} \sigma^2_e \\
\end{align*}
\begin{align*}
\gamma_k^{(1)} &= 0, \qquad \forall k \geq 3
\end{align*}
Fonction d'autocorrélation
\begin{align*}
\rho_1^{(1)} &= \frac{\gamma_1^{(1)}}{\gamma_0^{(1)}} \\
&= \frac{\frac{-3}{16}\sigma^2_e}{\frac{9}{8}\sigma^2_e} \\
&= \frac{-1}{6} \\
\end{align*}
\begin{align*}
\rho_2^{(1)} &= \frac{\gamma_2^{(1)}}{\gamma_0^{(1)}} \\
&= \frac{\frac{-1}{4}\sigma^2_e}{\frac{9}{8}\sigma^2_e} \\
&= \frac{-2}{9} \\
\end{align*}
\item Second processus avec:
\begin{align*}
\theta_1 = -1 \quad \theta_2 = 4
\end{align*}
Fonction d'autocovariance
\begin{align*}
\gamma_0^{(2)} &= V[Y_t] \\
&= V[e_t]+V[e_{t-1}]+16V[e_{t-2}] \\
&= 18 \sigma^2_e
\end{align*}
\begin{align*}
\gamma_1^{(2)} &= Cov(Y_t,Y_{t-1}) \\
&= Cov(e_t + e_{t-1} - 4e_{t-2}, e_{t-1} + e_{t-2} - 4e_{t-3}) \\
&= Cov(e_{t-1},e_{t-1}) + Cov(-4e_{t-2},e_{t-2}) \\
&= (1 + (-1)(4))\sigma^2_e \\
&= -3\sigma^2_e \\
\end{align*}
\begin{align*}
\gamma_2^{(2)} &= Cov(Y_t,Y_{t-2}) \\
&= Cov(e_t + e_{t-1} - 4e_{t-2}, e_{t-2} + e_{t-3} - 4e_{t-4} )\\
&= Cov(- 4e_{t-2},e_{t-2}) \\
&= -4\sigma^2_e \\
\end{align*}
Fonction d'autocorrélation
\begin{align*}
\rho_1^{(2)} &= \frac{\gamma_1^{(2)}}{\gamma_0^{(2)}} \\
&= \frac{-3\sigma^2_e}{18\sigma^2_e} \\
&= \frac{-1}{6} \\
\end{align*}
\begin{align*}
\rho_2^{(2)} &= \frac{\gamma_2^{(2)}}{\gamma_0^{(2)}} \\
&= \frac{-4\sigma^2_e}{18\sigma^2_e} \\
&= \frac{-2}{9} \\
\end{align*}
\end{enumerate}

On remarque clairement que $\rho_1^{(1)} = \rho_1^{(2)}$ et $\rho_2^{(1)} = \rho_2^{(2)}$. 
La fonction d'autocovariance vaut toujours 1 pour $\rho_1$ et vaut 0 ailleurs.

\clearpage
\subsection{Estimateur des moments pour le processus AR(2)}
\begin{enumerate}
\item 
Les deux équations de Yule-Walker pour le modèle AR(2) sont les suivantes:
\begin{align*}
\rho_1 = \phi_1 + \rho_1 \phi_2 \\
\rho_2 = \rho_1\phi_1 + \phi_2
\end{align*}

En utilisant l'estimateur de la fonction d'autocovariance $\hat{\rho}$, on obtient alors:

\begin{align*}
\hat{\phi_1} = \frac{\hat{\rho_1}(1-\hat{\rho_2})}{1-\hat{\rho_1}^2} \\
\hat{\phi_2} = \frac{\hat{\rho_2} - \hat{\rho_1}^2}{1-\hat{\rho_1}^2}
\end{align*}


\item

\begin{Schunk}
\begin{Sinput}
> set.seed(123)
> (serie <- arima.sim(n = 10, list(ar = c(0.5,-0.25))))
\end{Sinput}
\begin{Soutput}
Time Series:
Start = 1 
End = 10 
Frequency = 1 
 [1]  1.1617660  0.6981185  0.1693004 -0.6457205  1.4217278  1.3701445
 [7] -1.6369769 -0.4596686 -0.2933815 -1.0995973
\end{Soutput}
\begin{Sinput}
> acf.serie <- acf(serie,type="correlation",plot=FALSE)$acf[2:3]
> phi1 <- acf.serie[1]*(1-acf.serie[2]) / (1-acf.serie[1]^2)
> phi2 <- (acf.serie[2] - acf.serie[1]^2) / (1-acf.serie[1]^2)
\end{Sinput}
\end{Schunk}

On obtient $\hat{\rho_1} = 0.07455$ et $\hat{\rho_2} = -0.28041$. 
Ce qui nous donne les paramètres du modèle AR(2) suivants:
$\hat{\phi_1} = 0.09599$ et 
$\hat{\phi_2} = -0.28757$.
\end{enumerate}

\subsection{Terme d'erreur du processus ARMA(2,1) (Théorique)}

\begin{enumerate}
\item
  On représente le processus ARMA(2,1) sous la forme suivante:
  \begin{align*}
    y_t = \mu + \gamma_1 y_{t-1} + \gamma_2 y_{t-2} + \epsilon_t - \theta\epsilon_{t-1}
  \end{align*}
  
  En utilisant l'opérateur de rétrodécalage $B$, on peut exprimer cette équation sous la forme suivante:
  \begin{align*}
    (1-\theta B)\epsilon_t = y_t - \mu - \gamma_1 y_{t-1} - \gamma_2 y_{t-2}
  \end{align*}
  
  On divise ensuite de chaque côté par $(1-\theta B)$, pour obtenir:
  \begin{align*}
    \epsilon_t = \frac{1}{(1-\theta B)} \left(y_t - \mu - \gamma_1 y_{t-1} - \gamma_2 y_{t-2}\right)
  \end{align*}
  
  L'hypothèse de stationnarité nous permet de poser que $| \theta | < 1$, ce qui nous permet d'utiliser la série géométrique définie comme suit:
  \begin{align*}
    \frac{1}{1-\theta B} = \sum_{i=0}^{\infty} \theta^i B^i
  \end{align*}
  
  On obtient donc que 
  \begin{align*}
    \epsilon_t = \sum_{i=0}^{\infty} \theta^i B^i \left(y_t - \mu - \gamma_1 y_{t-1} - \gamma_2 y_{t-2}\right)
  \end{align*}
  
  En appliquant l'opérateur de rétrodécalage à la parenthèse, on obtient la solution:
  \begin{align*}
    \epsilon_t = \sum_{i=0}^{\infty} \theta^i \left(y_{t-i} - \mu - \gamma_1 y_{t-i-1} - \gamma_2 y_{t-i-2} \right)
  \end{align*}

\item
  
  On exprime l'équation précédente en fonction de $y_t$:
  \begin{align*}
    y_t &= \gamma_1 y_{t-1} + \gamma_2 y_{t-2} - \sum_{i=1}^{\infty} \theta^i \left(y_{t-i} - \gamma_1 y_{t-i-1} - \gamma_2 y_{t-i-2} \right) + \epsilon_t + \frac{\mu}{1-\theta} \\
    &= \gamma_1 y_{t-1} + \gamma_2 y_{t-2} - \left[\theta\left(y_{t-1}-\gamma_1 y_{t-2} - \gamma_2 y_{t-3} \right) + \sum_{i=2}^{\infty} \theta^i \left(y_{t-i} - \gamma_1 y_{t-i-1} - \gamma_2 y_{t-i-2} \right)\right] + \epsilon_t + \frac{\mu}{1-\theta}  \\
    &= (\gamma_1 - \theta) y_{t-1} - \sum_{i=2}^{\infty} \left(\theta_i+\gamma_1\theta^{i-1} - \gamma_2\theta_{i-2}\right) y_{t-i} + \epsilon_t + \frac{\mu}{1-\theta}
  \end{align*}
  Ce qui correspont à la forme autorégressive AR($\infty$) suivante:
  \begin{align*}
    y_t = c + \sum_{i=1}^{\infty} \pi_i y_{t-i} + \epsilon_t
  \end{align*}
  Avec les paramètres
  \begin{align*}
    c &= \frac{\mu}{1-\theta} \\
    \pi_1 &= (\gamma_1 - \theta) \\
    \pi_i &= -\left(\theta_i+\gamma_1\theta^{i-1} - \gamma_2\theta_{i-2}\right),\quad i=2,3,\ldots
  \end{align*}
  
\end{enumerate}
\clearpage
\input{cc}
\end{document}
